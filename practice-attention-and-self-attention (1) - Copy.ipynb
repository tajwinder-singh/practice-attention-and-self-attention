{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Input, Embedding, Dot, Concatenate, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "import sentencepiece as sp\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "!pip install fr_core_news_sm\n",
    "from datasets import load_dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = load_dataset(\"opus_books\", \"en-fr\")\n",
    "print(data)\n",
    "\n",
    "df = pd.DataFrame(data['train'])\n",
    "df['english'] = df['translation'].apply(lambda x: x['en'])\n",
    "df['french'] = df['translataion'].apply(lambda x: x['fr'])\n",
    "\n",
    "df2 = df[['english', 'french']]\n",
    "df3 = df2.head(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nlp_en = spacy.load('en_core_web_sm', disable = ['parser'])\n",
    "nlp_fr = spacy.load('fr_news_core_sm', disable = ['parser'])\n",
    "\n",
    "def smart_case(sentences, nlp, batch_size = 1000):\n",
    "    processed = []\n",
    "    \n",
    "    for doc in tqdm(nlp.pipe(sentences, batch_size = batch_size, n_process = -1), total = len(sentences)):\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            if token.ent_type_ or token.pos_ in ['PROPN'] or token.text.isupper():\n",
    "                tokens.append(token.text)\n",
    "            else:\n",
    "                tokens.append(token.text.lower())\n",
    "        processed.append(' '.join(tokens))\n",
    "    return processed\n",
    "\n",
    "\n",
    "english_sentences = smart_case(df['english'].tolist(), nlp_en)\n",
    "french_sentences = smart_case(df['french'].tolist(), nlp_fr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"english_smartCased_sentences.json\", \"w\", encoding = 'utf-8') as f:\n",
    "    json.dump(english_sentences, f, ensure_ascii = False, indent = 2)\n",
    "\n",
    "with open(\"french_smartCased_sentences.json\", \"w\", encoding = 'utf-8') as f:\n",
    "    json.dump(french_sentences, f, ensure_ascii = False, indent = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "french_input_sentences = ['<start> ' + s for s in french_sentences]\n",
    "french_output_senteces = [s + ' <end>' for s in french_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"eng.txt\", \"w\", encoding = 'utf-8') as f:\n",
    "    for s in english_sentences:\n",
    "        f.write(s + \"\\n\")\n",
    "\n",
    "with  open(\"fr.txt\", \"w\", encoding = 'utf-8') as f:\n",
    "    for s in french_sentences:\n",
    "        f.write(s + \"\\n\")\n",
    "\n",
    "sp.SentencePieceTrainer.Train(input = \"en.txt\", vocab_size = 3000, model_prefix = \"eng_sp\", model_type = \"bpe\")\n",
    "sp.SentencePieceTrainer.Train(input = \"fr.txt\", vocab_size = 3000, model_prefix = \"fr_sp\", model_type = \"bpe\")\n",
    "\n",
    "eng_sp = sp.SentencePieceProcessor()\n",
    "eng_sp.Load(\"eng_sp.model\")\n",
    "\n",
    "fr_sp = sp.SentencePieceProcessor()\n",
    "fr_sp.Load(\"fr_sp.model\")\n",
    "\n",
    "fr_in_seq = [fr_sp.EncodeAsIds(s) for s in french_input_sentences]\n",
    "fr_out_seq = [fr_sp.EncodeAsIds(s) for s in french_output_sentences]\n",
    "eng_seq = [eng_sp.EncodeAsIds(s) for s in english_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "length = [len(s) for s in english_sentences]\n",
    "eng_max_len = np.percentile(length, 90)\n",
    "\n",
    "lengths2 = [len(s) for s in french_sentences]\n",
    "fr_max_len = np.percentile(lengths2, 90)\n",
    "\n",
    "eng_vocab = eng_sp.GetPieceSize()\n",
    "fr_vocab = fr_sp.GetPieceSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eng_seq = pad_sequences(eng_seq, maxlen = eng_max_len, padding = 'post')\n",
    "fr_in_seq = pad_sequences(fr_in_seq, maxlen = fr_max_len, padding = 'post')\n",
    "fr_out_seq = pad_sequences(fr_out_seq, maxlen = fr_max_len, padding = 'post')\n",
    "\n",
    "\n",
    "# ------- Parameters ------\n",
    "embedding_dim = 64\n",
    "units = 128\n",
    "dropout = 0.2\n",
    "\n",
    "\n",
    "# ------ Encoder -------\n",
    "encoder_input = Input(shape = (eng_seq.shape[1], ))\n",
    "encoder_emb = Embedding(eng_vocab, embedding_dim, mask_zero = True)(encoder_input)\n",
    "encoder_emb = Dropout(dropout)(encoder_emb)\n",
    "\n",
    "encoder_biLstm = Bidirectional(LSTM(units, return_sequences = True, return_state = True, dropout = dropout, recurrent_dropout = dropout))\n",
    "encoder_output, forward_h, forward_c, backward_h, backward_c = encoder_biLstm(encoder_emb)\n",
    "\n",
    "combined_h = Concatenate()([forward_h, backward_h])\n",
    "combined_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "\n",
    "# -------- Decoder --------\n",
    "decoder_input = Input(shape = (fr_seq_out.shape[1]), )\n",
    "decoder_emb = Embedding(fr_vocab, embedding_dim, mask_zero = True)(decoder_input)\n",
    "decoder_emb = Dropout(dropout)(decoder_emb)\n",
    "\n",
    "decoder_lstm = LSTM(units*2, return_sequences = True, return_state = True, dropout = dropout)\n",
    "decoder_output, _, _ = decoder_lstm(decoder_emb, initial_state = [combined_h, combined_c])\n",
    "\n",
    "\n",
    "\n",
    "# -------- Luong (Multiplicative) Attention --------- \n",
    "# This attention multiplies the hiddenstates. Bahadnau adds the hidden states. We use Luong only since it is faster and simple.\n",
    "score = Dot(axis=[2, 2])([decoder_output, encoder_output]) # 'Since we know that the output of the encoder-decoder when return_sequences = True is: (batch_size, timesteps, units), so, the dot product of units will be there because units carries the output probabilitites. Therefore the dot product will be caluculated for units(since axis = 2)\n",
    "attention_weights = Activation('softmax', name = 'attention_weights')(score) # Calculates attention weight(alpha) using softmax.\n",
    "\n",
    "\n",
    "# Context Vector\n",
    "context_vector = Dot(axis=[2, 1])([attention_weights, encoder_output]) # To know the reason see the below notes.\n",
    "\n",
    "\n",
    "# Combined Context\n",
    "decoder_combined_context = Concatenate(axis = -1)([context_vector, decoder_output]) # 'axis=-1' because, the last axis which is the no. of units will be concatenated(see the below logic).\n",
    "\n",
    "# Output Layer\n",
    "decoder_dense = Dense(fr_vocab, activation = 'softmax')\n",
    "decoder_output_final = decoder_dense(decoder_combined_context) # Now, giving the attention outputs instead of giving the LSTM outputs.\n",
    "\n",
    "\n",
    "# Model Training\n",
    "model = Model([encoder_input, decoder_input], decoder_output_final)\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()\n",
    "model.fit([eng_seq, fr_in_seq], np.expand_dims(fr_out_seq, -1), epochs = 100, validation_split = 0.1)\n",
    "\n",
    "\n",
    "# ------- Inference Model ---------\n",
    "# Encoder Inference Model\n",
    "encoder_model = Model(encoder_input, [encoder_output, combined_h, combined_c]) # Since the decoder want the encoder's output state at each timestep that's why gave the encoder_output.\n",
    "encoder_output_input = Input(shape = (eng_max_len, units*2)) # See the below notes to know the logic the of this.\n",
    "\n",
    "\n",
    "# Decoder Inference Model\n",
    "decoder_state_h = Input(shape = (units*2, ))\n",
    "decoder_state_c = Input(shape = (units*2, ))\n",
    "decoder_input_inf = Input(shape = (1, )) # One word at a time.\n",
    "decoder_emb2 = Embedding(fr_vocab, embedding_dim, mask_zero = True)(decoder_input_inf)\n",
    "decoder_output2, decoder_output_h, decoder_output_c = decoder_lstm(decoder_emb2, initial_state = [decoder_state_h, decoder_state_c])\n",
    "\n",
    "\n",
    "# Attention at inference\n",
    "score2 = Dot(axis=(2,2))([decoder_output2, encoder_output_input]) # You have to always write the encoder_output at after decoder output.\n",
    "attention_weights2 = Activation('softmax')(score2)\n",
    "context_vector2 = Dot(axis = (2, 1))([attention_weights2, encoder_output_input])\n",
    "decoder_combined_context2 = Concatenate()([context_vector2, decoder_output2])\n",
    "\n",
    "decoder_final_output2 = decoder_dense(decoder_combined_context2)\n",
    "\n",
    "decoder_model = Model([decoder_input_inf, decoder_state_h, decoder_state_c, encoder_output_input], # Passing encoder_output_input is mandatory since the decoder needs it calculate context vectors.\n",
    "                     [decoder_final_output2, decoder_output_h, decoder_output_c, attention_weights]) # Passing attention_weights for visualization purposes only. It has nothting to do with the model.\n",
    "\n",
    "\n",
    "def translate_sentence(sentence):\n",
    "    sentence = ' '.join(smart_case([sentence], nlp_en))\n",
    "    seq = eng_sp.EncodeAsIds(sentence)\n",
    "    seq = pad_sequences([seq], padding = 'post', maxlen = eng_max_len)\n",
    "    en_output, state_h_, state_c_ = encoder_model.predict(seq)\n",
    "    start_id = eng_sp.PieceToId('<start>')\n",
    "    stop = False\n",
    "    input_id = np.array([[start_id]])\n",
    "    decoded_sentence = []\n",
    "    \n",
    "    while not stop:\n",
    "        decoder_output_, decoder_output_h_, decoder_output_c_ = decoder_model.predict([input_id, state_h_, state_c_, en_output]) # order must be same.\n",
    "        output_id = np.argmax(decoder_output_[0, -1, :]) # Taking the final timestep as output word(ID). Important: np.argmax() return the index of the maximum value not that value itself\n",
    "        word = fr_sp.DecodeIds(output_id)\n",
    "\n",
    "        if word == '<end>' or len(decoded_sentence) >= fr_max_len:\n",
    "            stop = True\n",
    "\n",
    "        else:\n",
    "            decoded_sentence.append(word)\n",
    "            input_id = np.array([[output_id]])\n",
    "            state_h_ = decoder_output_h_\n",
    "            state_c_ = decoder_output_c_\n",
    "\n",
    "    return ' '.join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Self Attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, MultiHeadAttention\n",
    "from import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "import spacy\n",
    "!pip install fr_core_news_sm\n",
    "from datasets import load_dataset\n",
    "import sentencepiece as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/dataset/english_smartCased_sentences.json\", encoding = 'utf-8') as f:\n",
    "    json.load(f)\n",
    "    english_sentences = f\n",
    "\n",
    "with open(\"/kaggle/input/dataset/french_smartCased_sentences.json\", encoding = 'utf-8') as f:\n",
    "    json.load(f)\n",
    "    french_sentences = f\n",
    "\n",
    "french_input_sentences = ['<start> ' + s for s in french_sentences]\n",
    "french_output_sentences = [s + ' <end>' for s in french_sentences]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eng_seq = pad_sequences(eng_seq, maxlen = eng_max_len, padding = 'post')\n",
    "fr_in_seq = pad_sequences(fr_seq, maxlen = fr_max_len, padding = 'post')\n",
    "fr_out_seq = pad_sequences(fr_out_seq, maxlen = fr_max_len, padding = 'post')\n",
    "eng_input = eng_seq.shape[1]\n",
    "fr_input = fr_in_seq.shape[1]\n",
    "\n",
    "# ------\n",
    "# Encoder\n",
    "# ------\n",
    "encoder_input = Input(shape = (eng_input, ))\n",
    "encoder_emb = Embedding(eng_vocab, embedding_dim, mask_zero = True)(encoder_input)\n",
    "encoder_biLstm = Bidirectional(LSTM(units, return_sequences = True, return_state = True, dropout = dropout))\n",
    "\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_biLstm(encoder_emb)\n",
    "\n",
    "combined_h = Concatenate()([forward_h, backward_h])\n",
    "combined_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "# Self Attention on Encoder outputs\n",
    "att_output = MultiHeadAttention(num_head = 4, key_dim = units*2)(encoder_outputs, encoder_outputs) # See the below notes for knowing about everything. We have given encoder_outputs twice because, the Query, Key, and value will be calculated based on the encoder's output, and internally we need the separate outputs of encoder, so, internally in MultiHead() key, and value are passed as a same argument, but, for query it needs a separate argument to be passed. Query is the context of a current word, Key is the context of entire word and value is the embedding dimensions value.\n",
    "att_output = Dropout(dropout)(enc_att) # Applying dropout here will make the code less prone to overfitting. That's why haven't applied Dropout Earlier after encoder_biLstm.\n",
    "att_output = LayerNormalization()(enc_att + encoder_outputs) # For normalizing the layers. See the below notes for the entire information about this Luong Self Attention mechanism and paramteres used.\n",
    "\n",
    "\n",
    "# ------\n",
    "# Decoder\n",
    "# ------                              \n",
    "decoder_input = Input(shape = (fr_input, ))\n",
    "decoder_emb = Embedding(fr_vocab, embedding_dim, mask_zero = True)(decoder_input)\n",
    "decoder_lstm = LSTM(units, return_sequences = True, return_state = True, dropout = dropout)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_emb, initial_state = [combined_h, combined_c])\n",
    "\n",
    "# Cross Attention(decoder queries and encoder outputs)\n",
    "cross_att = MultiHeadAttention(num_head = 4, key_dim = units*2)(decoder_outputs, att_output)\n",
    "cross_att = Dropout(dropout)(cross_att)\n",
    "cross_att = LayerNormalization()(cross_att + decoder_outputs)\n",
    "\n",
    "# Output Dense(FFNN)\n",
    "decoder_dense = Dense(fr_vocab, activation = 'softmax')\n",
    "decoder_outputs = decoder_dense(cross_att)\n",
    "\n",
    "\n",
    "# ----------\n",
    "# Seq2Seq Model\n",
    "# ----------\n",
    "model = Model([encoder_input, decoder_input], decoder_outputs)\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()\n",
    "model.fit([eng_seq, fr_in_seq], np.expand_dims(fr_out_seq , -1), epochs = 100, validation_split = 0.2)\n",
    "\n",
    "\n",
    "# ---------------\n",
    "# Inference Model\n",
    "# ---------------\n",
    "# Encoder Inference Model\n",
    "encoder_model = Model(encoder_input, [encoder_outputs, combined_h, combined_c])\n",
    "enc_att_input = Input(shape = (eng_max_len, units*2)) # Just like we did in simple attention, but, we'll name it as enc_att_input.\n",
    "\n",
    "# Deocder Inference Model\n",
    "decoder_input2 = Input(shape = (1, ))\n",
    "decoder_input_h = Input(shape = (units*2, ))\n",
    "decoder_input_c = Input(shape = (units*2, ))\n",
    "\n",
    "decoder_emb2 = Embedding(fr_vocab, embedding_dim, mask_zero = True)(decoder_input)\n",
    "decoder_outputs2, decoder_output_h, decoder_output_c = decoder_lstm(decoder_emb2, initial_state = [decoder_input_h, decoder_input_c])\n",
    "\n",
    "# Cross Attention:\n",
    "cross_att2 = MultiHeadAttention(num_heads = 4, key_dim = units*2)(decoder_outputs2, enc_att_input)\n",
    "cross_att2 = LayerNormalizatoin()(cross_att2 + decoder_outputs2)\n",
    "\n",
    "decoder_outputs2_inf = decoder_dense(cross_att2)\n",
    "decoder_model = Model([decoder_input2, decoder_input_h, decoder_input_c, encoder_output_input], \n",
    "                      [decoder_output2_inf, decoder_output_h, decodeer_output_c])\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8519274,
     "sourceId": 13422654,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
